{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import requests\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check path of the chromedriver\n",
    "#!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "\n",
    "# def init_browser():\n",
    "#     executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "#     return Browser(\"chrome\", **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign path for chromedriver\n",
    "executable_path = {'executable_path': '/c/Program Files/Chrome_extension/chromedriver'}\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium==3.5.0\n",
    "# splinter==0.7.6\n",
    "\n",
    "# URL of the website being scraped\n",
    "url= 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve page with the requests module\n",
    "# response = requests.get(url)\n",
    "\n",
    "# Wait for 5 seconds\n",
    "time.sleep(5)\n",
    "\n",
    "html= browser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "# soup = bs(response.text, 'html.parser')\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are returned as an iterable list\n",
    "# news_title = soup.find_all('div', class_=\"content_title\")\n",
    "# print(news_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and choose which news title you want to access and get the latest title\n",
    "latest_news_title = soup.find_all('div', class_=\"content_title\")[1].text\n",
    "print(latest_news_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_paragraph= soup.find_all('div', class_=\"content_title\")\n",
    "# print(news_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and choose which news paragraph you want to access and get the latest news paragraph\n",
    "latest_news_paragraph= soup.find_all('div', class_=\"article_teaser_body\")[0].text\n",
    "print(latest_news_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website being scraped\n",
    "url_2= 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url_2)\n",
    "# Wait for 5 seconds\n",
    "time.sleep(5)\n",
    "\n",
    "html_2= browser.html\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "\n",
    "soup = bs(html_2, 'html.parser')\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featured image url: https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA23857_hires.jpg\n"
     ]
    }
   ],
   "source": [
    "# Get image url for featured image.\n",
    "base_url = \"https://www.jpl.nasa.gov\"\n",
    "featured_image_url= soup.find('li', class_='slide').a['data-fancybox-href']\n",
    "featured_image_url= base_url + featured_image_url\n",
    "print(f\"Featured image url: {featured_image_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InSight sol 509 (2020-05-02) low -93.2ºC (-135.7ºF) high -2.2ºC (28.1ºF)\n",
      "winds from the SW at 5.1 m/s (11.5 mph) gusting to 17.9 m/s (40.0 mph)\n",
      "pressure at 6.80 hPa\n"
     ]
    }
   ],
   "source": [
    "# URL of the website being scraped\n",
    "url_5= \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(url_5)\n",
    "# Wait for 10 seconds\n",
    "time.sleep(5)\n",
    "\n",
    "html_5= browser.html\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html_5, 'html.parser')\n",
    "\n",
    "##First Method that worked\n",
    "# tweet= soup.find('article', class_= 'css-1dbjc4n')\n",
    "# mars= tweet.find_all('span', class_= 'css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0')\n",
    "## Grab text of most recent tweet\n",
    "# mars1= mars[-3]\n",
    "# print(mars1.text)\n",
    "\n",
    "\n",
    "#Easier Way\n",
    "# Search for most recent tweet\n",
    "tweet= soup.find_all('div', attrs={\"lang\":\"en\", \"dir\":\"auto\", \"class\":\"css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\"})\n",
    "tweet= tweet[-2].text\n",
    "print(tweet)                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website being scraped\n",
    "url_3= 'https://space-facts.com/mars/'\n",
    "browser.visit(url_3)\n",
    "# Wait for 5 seconds\n",
    "time.sleep(5)\n",
    "\n",
    "html_3= browser.html\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "\n",
    "soup = bs(html_3, 'html.parser')\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the table being scraped\n",
    "tables= pd.read_html(url_3)\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= tables[0]\n",
    "df.columns = [\"Measurement\", \"Value\"]\n",
    "df.set_index(\"Measurement\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to HTML\n",
    "html_table= df.to_html(\"templates/mars_facts_table.html\")\n",
    "html_table = df.to_html()\n",
    "print(html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website being scraped\n",
    "url_4= 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(url_4)\n",
    "# Wait for 5 seconds\n",
    "time.sleep(5)\n",
    "\n",
    "html_4= browser.html\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "\n",
    "soup = bs(html_4, 'html.parser')\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hemisphere name and image url for the full resolution image.\n",
    "hemispheres = soup.find_all('div', class_='item')\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for hemisphere in hemispheres:\n",
    "    text = hemisphere.find('h3').text\n",
    "#     split_text = text.split('Enhanced')\n",
    "#     title = split_text[0]\n",
    "    title= text\n",
    "    browser.click_link_by_partial_text(text)\n",
    "    hemisphere_page_html = browser.html\n",
    "    soup = bs(hemisphere_page_html, \"html.parser\")\n",
    "    downloads = soup.find('div', class_=\"downloads\")\n",
    "    img_url = downloads.a[\"href\"]\n",
    "    hemisphere_dict = { \n",
    "        \"title\": title, \n",
    "        \"img_url\": img_url\n",
    "    }\n",
    "    hemisphere_image_urls.append(hemisphere_dict)\n",
    "    browser.back()\n",
    "    \n",
    "print(hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
